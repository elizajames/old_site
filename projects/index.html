---
layout: default
title: Projects Completed
---
<div class="blurb">
	<p>This page is divided into two broad categories--my work with <a href="#datavis">computation and data visualization</a> of cultural heritage data and metadata and my work doing <a href="#teachingoutreach">teaching and outreach</a>, usually informed by technology and usage of primary sources.</p>
	
	<h2 id="datavis">Representative Projects-Computation and Data Visualization</h2>
	
	<h3>Visualizing Francis Pierpont's Telegram Network</h3>
	<p>This project visualized the communication and geospatial network of individuals who sent telegrams to Francis Pierpont, first governor of West Virginia, during the Civil War. These 850 telegrams were digitized by West Virginia University in 2011 and included granular transcriptions of the sender name, sender location, recipient, recipient location, message sent, and date. These metadata components make it possible for researchers to easily work with the telegrams as data. First, however, I had to get that data. WVU uses a content management system that creates a stable and iterative numerical URI for each digitized item but did not have an API through which to get that data. I wrote code in Python that allowed me to automate the scraping of metadata from each page. Cleaning the data involved some manual labor because locations were transcribed as written, including temporary camp locations that are no longer extant, rather than mapped to specific locations or coordinates in the present day. I then used additional automated tools to get coordinates for each location based on the newly standardized location names.</p>
	<p>Initially, I chose to visualize this data set with TimeMapper, which allows individuals to interact with the information geospatially and temporally. However, I found this approach was inadequate because the frequency of telegrams sent from a particular location was not visualized, making it difficult for the viewer to understand which locations or individuals Pierpont corresponded with most frequently. Additionally, I felt that the network of communicants was not adequately visualized by isolated pins alone. However, I do feel that TimeMapper is a good tool for those who wish to emphasize discovery of the content in the collection over time and space.</p>
	<img src="/images/PierpontNetwork.png" alt="TimeMapper Screenshot of map, timeline, and information about the Francis Pierpont Telegram Collection at West Virginia University." class="responsive">
	<p>I chose to move this data to Palladio, which allowed me to better visualize the frequency of telegrams sent as well as the network of individuals who sent these telegrams. Creating a network of senders and sender locations (second image below) allowed me to notice connections between various individuals who existed in the same time and space in a way that was nearly impossible when viewing the telegrams as discrete items.</p>
	<img src="/images/PierpontPalladioMap.png" alt="Map showing the locations of telegram senders with nodes sized to match the number of telegrams sent from that location." class="responsive">
	<img src="/images/PierpontPalladioNetwork.png" alt="Map showing a network of telegram sender names and locations with locations as the nodes. Nodes are sized by number of telegrams." class="responsive">

	<h3>Mapping Looking at Appalachia</h3>
	<p>I have admired the work of Roger May for a long time, and when he put out a call on Twitter for help mapping the Looking at Appalachia project, I had no choice but to respond. The Looking at Appalachia project is composed of more than 800 photos contributed from individuals throughout the Appalachian region to demonstrate the diversity and complexity of the Appalachian experience. On the project website, the photos are housed in an image carousel and organized by state of origin and feature only plain text unstandardized metadata about each photo. I wrote a Python program to visit the page for each state and crawl the page for the URL and string of metadata associated with each of the images. Then, I used regular expressions to parse out data for the photographer, date, URL, and location of each photograph. Because this data was not created by a cataloger or other metadata expert, some manual and automated cleanup was needed in Excel to clean and map locations to federal county FIPS codes. Specifically, Virginia has some unique geographic entities that are independent cities within counties that defied standardized methods. Then, the data was imported into Datawrapper for visualization and dissemination purposes.</p>
	<p>Creating this map allowed me to help Roger more concretely see what areas of Appalachia are more represented by the project and help determine what areas could benefit from outreach. Additionally, this map helps demonstrate that Looking at Appalachia depicts a broad geographic range of Appalachian experiences.</p>
	<img src="/images/LookingAtAppalachia.png" alt="Map showing the frequency and distribution of photograph submissions to the Looking at Appalachia project." class="responsive">
	
	<h3>West Virginia FSA-OWI Photos</h3>
	<p> I used Python and the Library of Congress API to examine metadata of photographs taken in West Virginia from the Farm Security Administration/Office of War Information (FSA-OWI) Black-and-White Negatives Collection. To start, I pulled the URL, title, date, creator, location, and subject data for the 2525 search results. I then used Python and the Mapquest API to assign latitude and longitude coordinates to each location subject heading to make each photograph mappable. I then imported the data into Palladio to create a network visualization showing where photographer coverage overlapped within the state as well as a map showing the most photographed locations. I also created a graphical breakdown of what photographers captured the most photographs. However, I was especially interested in looking at the content of the photos. Given that the subject data present for collection materials was composed only of geographic subject headings, I resorted to using Voyant Tools to analyze the more detailed title data to determine topical subject matter. After correcting for the terms “west virginia’, “untitled”, “photo”, and other similarly broad descriptive terms, I was able to determine that approximately 50% of the photographs mentioned terms related to “coal”, “mining”, or “company towns”. Currently, I am working to use the data to identify groupings of photographs for the creation of primary source sets and related technology informed analysis activities that leverage the large amount of data for teachers of West Virginia and labor history.</p>
	<p>One issue I've noted with the data in this set is the lack of consistent location information. Additionally, many of the locations in the data set no longer exist in many common mapping APIs because they were company towns that are no longer populated.</p>
	<img src="/images/Top20Terms.png" alt="Top 20 most frequently occurring words in the titles of FSA-OWI photos taken in West Virginia." class="responsive">
	<img src="/images/WVPhotographers.png" alt="Percentage breakdown of photographers who took photos in West Virginia with the FSA-OWI." class="responsive">
	
	<h3>Data Manipulation and Transformation</h3>
	<p>Data created in archives can be very subjective and reliant on the specific individual who creates the data, a trend especially apparent in smaller organizations over time. In conjunction with biased individuals and cataloging standards, gaps in quality of metadata creation can also occur when transitioning from one archivist or technology to another. I’ve used Python, OpenRefine, and Excel to automate the standardization and cleanup of archives data to allow for more consistent use. Specific projects in this area include:</p>
	<ul><li class="weirdlist">Standardizing data from finding aids by creating EAD and XML authoring programs to allow non-technically minded archivists and individuals to aid in manual cleanup while still creating data that can be used in a modular and computational way.</li>
	<li class="weirdlist">Usage of Python to clean, crosswalk, and migrate legacy metadata describing digital objects into being compliant with new metadata standards or platform needs.</li>
	<li class="weirdlist">Parsing plain text, MARC, or XML metadata into CSV or other forms as needed for a particular software to allow for data visualization or user research.</li></ul>

	
	<h3>Exploring the Southern Courier (In Progress)</h3>
	<p><i>The Southern Courier</i> was a newspaper published out of Montgomery, Alabama during the peak of the Civil Rights Movement. All 177 issues of <i>The Southern Courier</i> are digitized and available online but are not full text searchable. To complement this valuable collection, the staff photographer for the newspaper, Jim Peppler, donated 13,000 negatives to the Alabama Department of Archives and History. The latter collection is fully digitized and features full and rich metadata for each photograph using standardized vocabularies, making it perfect for usage in data visualizations or analyses.</p>
	<p>Putting large numbers of photos in a content management system and asking users to input keywords isn’t the most ideal. I am working on this project to explore how cultural heritage institutions can make it easier for users to explore and enjoy digital materials recreationally rather than for a specific purpose. For this project, I’m especially interested in determining how treating digital collection items as data can allow for improved discovery of content by non-academics and ultimately enhance the use of these two unique collections.</p>
	
	<h2 id="teachingoutreach">Representative Projects-Teaching and Outreach</h2>
	
	<h3>Vietnam Veteran Oral Histories as Data</h3>
	<p>While working with large humanistic data sets can create more broadly applicable scholarship, working with small sets of data in one-shot instruction sessions can help students begin to understand the value of working computationally with materials that are traditionally not viewed through a computational lens. For instance, when viewing the interrelated subject data associated with twenty-nine oral histories by Vietnam veterans, students can visibly see the significant overlap in content discussed by veterans and identify broader themes. Using Palladio to visualize the overlap in content makes it possible for students to discover and compare oral histories that focus on these themes, ultimately creating higher quality scholarship. The visualization below was used in a session of a First Year Seminar course for undergraduate students that focused on analyzing propaganda in the Vietnam War as a way of fostering critical thinking and information literacy skills.</p>
	<p>The below graph is difficult to read as-is, but is used to navigate the data when zoomed in. When zoomed out, however, the viewer can see an “island” of data that is visually and topically separate from other oral histories. These oral histories are by conscientious objectors and individuals on the home front and only share one subject heading with veterans who were not conscientious objectors.</p>
	<img src="/images/VietnamVeterans.png" alt="Network showing the interelations of subject data between oral histories of Vietnam veterans." class="responsive">
	
	<h3>Lincoln's Pockets (and Your Pockets): Engaging students in grades 6-12 with the primary sources in their homes</h3>
	<p>This project was completed after a presentation with some wonderful folks at Teaching with Primary Sources - Western Region. Based on my portion of the presentation, which focused on transitioning to teaching with primary sources online, each presenter created a learning resource that used a framework I created for teaching in environments where students have access to variable levels of technology and familial support. My output consisted of three activities on an <a href="https://spark.adobe.com/page/Uo1dQPNjDDhxC/">Adobe Spark page</a> that expanded on the Library of Congress’ “Lincoln’s Pockets” activity and emphasized using digital and analog primary sources found in a student’s home to teach critical thinking and analysis skills that combat inequity in access to technology.</p>
	
	
</div><!-- /.blurb -->
